{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JOkp2dwtKUX"
      },
      "outputs": [],
      "source": [
        "# Блок 1\n",
        "!pip install -q transformers accelerate seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD30pkvWBaOp"
      },
      "outputs": [],
      "source": [
        "# Блок 2\n",
        "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n",
        "                          DataCollatorForTokenClassification, TrainingArguments, Trainer)\n",
        "from seqeval.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeRU6NaU0t7_"
      },
      "outputs": [],
      "source": [
        "# Блок 3\n",
        "import pandas as pd, ast, re\n",
        "\n",
        "USE_SYNT = True\n",
        "\n",
        "train_orig = pd.read_csv(\"train.csv\", sep=\";\", quotechar='\"', engine=\"python\")\n",
        "\n",
        "assert {\"sample\", \"annotation\"} <= set(train_orig.columns), \"Ожидаем sample и annotation в train_df\"\n",
        "\n",
        "if USE_SYNT:\n",
        "    synthetic_train = pd.read_csv(\"synthetic_train.csv\", sep=\";\", quotechar='\"', engine=\"python\")\n",
        "\n",
        "    assert {\"sample\", \"annotation\"} <= set(synthetic_train.columns), \"Ожидаем sample и annotation в submission_df\"\n",
        "\n",
        "    train_df = pd.concat([train_orig, synthetic_train], ignore_index=True)\n",
        "\n",
        "    train_df = train_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "    print(f\"train.csv: {len(train_orig)} строк\")\n",
        "    print(f\"synthetic_train.csv: {len(synthetic_train)} строк\")\n",
        "    print(f\"Объединённый train_df: {len(train_df)} строк\")\n",
        "else:\n",
        "    train_df = train_orig\n",
        "    print(f\"train.csv без синтетики: {len(train_df)} строк\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibeZxr4V0wI6"
      },
      "outputs": [],
      "source": [
        "# Блок 4\n",
        "ALLOWED = {\"TYPE\",\"BRAND\",\"VOLUME\",\"PERCENT\"}\n",
        "\n",
        "def clean_ann(s: str):\n",
        "    try:\n",
        "        parsed = ast.literal_eval(s)\n",
        "    except Exception:\n",
        "        return []\n",
        "    out=[]\n",
        "    for x in parsed:\n",
        "        if isinstance(x,(list,tuple)) and len(x)==3:\n",
        "            s0,e0,t = x\n",
        "            if t != \"O\":\n",
        "                t = re.sub(r\"^(B-|I-)\", \"\", str(t))\n",
        "                if t in ALLOWED:\n",
        "                    try:\n",
        "                        out.append([int(s0), int(e0), t])\n",
        "                    except: pass\n",
        "    return out\n",
        "\n",
        "train_df[\"entities\"] = train_df[\"annotation\"].astype(str).apply(clean_ann)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6EKDVzU0xQr"
      },
      "outputs": [],
      "source": [
        "# Блок 5\n",
        "model_name = \"cointegrated/rubert-tiny2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "labels = [\"O\",\"B-TYPE\",\"I-TYPE\",\"B-BRAND\",\"I-BRAND\",\"B-VOLUME\",\"I-VOLUME\",\"B-PERCENT\",\"I-PERCENT\"]\n",
        "label2id = {l:i for i,l in enumerate(labels)}\n",
        "id2label = {i:l for l,i in label2id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7O03E130zrE"
      },
      "outputs": [],
      "source": [
        "# Блок 6\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class NERDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=256):\n",
        "        self.samples = df[\"sample\"].tolist()\n",
        "        self.entities = df[\"entities\"].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.samples[idx]\n",
        "        spans = self.entities[idx]\n",
        "\n",
        "        ents = []\n",
        "        L = len(text)\n",
        "        for s0, e0, t in spans:\n",
        "            s0 = int(s0); e0 = int(e0)\n",
        "            if 0 <= s0 < e0 <= L:\n",
        "                ents.append((s0, e0, str(t)))\n",
        "        ents.sort(key=lambda x: (x[0], x[1]))\n",
        "\n",
        "        enc = self.tokenizer(\n",
        "            text,\n",
        "            return_offsets_mapping=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len\n",
        "        )\n",
        "        offsets = enc[\"offset_mapping\"]\n",
        "\n",
        "        labels_ids = []\n",
        "        for (st, en) in offsets:\n",
        "            if st == en:\n",
        "                labels_ids.append(-100)\n",
        "                continue\n",
        "\n",
        "            lab = \"O\"\n",
        "            for s0, e0, t in ents:\n",
        "                if max(st, s0) < min(en, e0):\n",
        "                    lab = f\"B-{t}\" if st == s0 else f\"I-{t}\"\n",
        "                    break\n",
        "\n",
        "            labels_ids.append(label2id.get(lab, 0))\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(enc[\"input_ids\"], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(enc[\"attention_mask\"], dtype=torch.long),\n",
        "            \"labels\": torch.tensor(labels_ids, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "perm = rng.permutation(len(train_df))\n",
        "cut = int(0.9*len(train_df))\n",
        "tr_idx, va_idx = perm[:cut], perm[cut:]\n",
        "train_ds = NERDataset(train_df.iloc[tr_idx].reset_index(drop=True), tokenizer, max_len=256)\n",
        "val_ds   = NERDataset(train_df.iloc[va_idx].reset_index(drop=True), tokenizer, max_len=256)\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name, num_labels=len(labels), id2label=id2label, label2id=label2id\n",
        ")\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "from seqeval.metrics import f1_score\n",
        "from seqeval.scheme import IOB2\n",
        "\n",
        "def compute_metrics(p):\n",
        "    logits, labels_arr = p\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    true_preds, true_labels = [], []\n",
        "    for p_row, l_row in zip(preds, labels_arr):\n",
        "        p_seq, l_seq = [], []\n",
        "        for p_i, l_i in zip(p_row, l_row):\n",
        "            if l_i == -100:\n",
        "                continue\n",
        "            p_seq.append(id2label[int(p_i)])\n",
        "            l_seq.append(id2label[int(l_i)])\n",
        "        true_preds.append(p_seq)\n",
        "        true_labels.append(l_seq)\n",
        "    return {\"f1\": f1_score(true_labels, true_preds, zero_division=0, scheme=IOB2)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKWrRfGE05WS"
      },
      "outputs": [],
      "source": [
        "# Блок 7\n",
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "from seqeval.metrics import f1_score\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"ner_ckpt\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=12,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=3,\n",
        "\n",
        "    warmup_ratio=0.1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"adamw_torch\",\n",
        "    lr_scheduler_type=\"linear\",\n",
        "\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(trainer.evaluate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0UPnOB103jg"
      },
      "outputs": [],
      "source": [
        "# Блок 7.1\n",
        "# output_dir = \"/model\"\n",
        "\n",
        "# trainer.save_model(output_dir)\n",
        "# tokenizer.save_pretrained(output_dir)\n",
        "# trainer.save_state()\n",
        "# print(\"Best checkpoint:\", trainer.state.best_model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bNJruv01wKn"
      },
      "outputs": [],
      "source": [
        "# Блок 8.0\n",
        "# load_path = \"/model\"\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(load_path)\n",
        "# model = AutoModelForTokenClassification.from_pretrained(load_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atC05pJM3SNY"
      },
      "outputs": [],
      "source": [
        "# Блок 8\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "assert 'model' in globals() and 'tokenizer' in globals(), \"Сначала загрузите model/tokenizer\"\n",
        "\n",
        "if 'labels' not in globals():\n",
        "    labels = [\"O\",\"B-TYPE\",\"I-TYPE\",\"B-BRAND\",\"I-BRAND\",\"B-VOLUME\",\"I-VOLUME\",\"B-PERCENT\",\"I-PERCENT\"]\n",
        "label2id = {l:i for i,l in enumerate(labels)}\n",
        "id2label  = {i:l for l,i in label2id.items()}\n",
        "\n",
        "CFG = {\n",
        "    \"use_margin_rule\": True,\n",
        "    \"margin_delta\": 0.06,\n",
        "    \"margin_delta_per_class\": {\n",
        "        \"TYPE\": 0.07,\n",
        "        \"BRAND\": 0.07,\n",
        "        \"VOLUME\": 0.02,\n",
        "        \"PERCENT\": 0.015,\n",
        "    },\n",
        "\n",
        "    \"numeric_overrides\": True,\n",
        "    \"trim_punct_on_spans\": True,\n",
        "    \"word_majority\": True,\n",
        "    \"word_inherit_prev\": True,\n",
        "    \"majority_threshold\": 0.58,\n",
        "    \"max_len\": 256,\n",
        "}\n",
        "\n",
        "ALLOWED_BASE = {\"TYPE\",\"BRAND\",\"VOLUME\",\"PERCENT\"}\n",
        "PUNCT = set(\";:,.!?()[]{}«»\\\"'—–-\")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval().to(DEVICE)\n",
        "\n",
        "RE_PERCENT = re.compile(r'(?<!\\d)(\\d{1,3}(?:[.,]\\d{1,2})?)\\s*%', re.I)\n",
        "RE_UNIT = r\"(?:мл|л|литр(?:а|ов)?|г|гр|грамм(?:а|ов)?|кг|шт|уп|упак|бут|бутыл(?:ка|ки|ок)|табл|таб|капс|порц|пак)\"\n",
        "RE_UNIT_DOT = r\"(?:мл\\.|л\\.|г\\.|гр\\.|шт\\.|уп\\.|таб\\.|капс\\.)\"\n",
        "RE_VOLUME1 = re.compile(rf'(?<!\\d)\\d+(?:[.,]\\d+)?\\s*(?:{RE_UNIT}|{RE_UNIT_DOT})(?!\\w)', re.I)\n",
        "RE_VOLUME2 = re.compile(rf'(?<!\\d)\\d+\\s*[x×х]\\s*\\d+\\s*(?:шт|уп|упак|таб|капс|пак)(?!\\w)', re.I)\n",
        "\n",
        "def _base(lab: str) -> str:\n",
        "    return \"O\" if lab==\"O\" else (lab.split(\"-\",1)[1] if \"-\" in lab else lab)\n",
        "\n",
        "def _repair_bio_token_sequence(token_labels: List[str]) -> List[str]:\n",
        "    out, prev = [], \"O\"\n",
        "    for lab in token_labels:\n",
        "        if lab == \"O\":\n",
        "            out.append(\"O\"); prev = \"O\"; continue\n",
        "        if \"-\" not in lab: lab = f\"B-{lab}\"\n",
        "        bio, typ = lab.split(\"-\", 1)\n",
        "        if bio == \"I\":\n",
        "            ok = prev.startswith((\"B-\",\"I-\")) and prev.split(\"-\",1)[1] == typ\n",
        "            lab = lab if ok else f\"B-{typ}\"\n",
        "        out.append(lab); prev = lab\n",
        "    return out\n",
        "\n",
        "def _trim_punct(text: str, s: int, e: int) -> Tuple[int,int]:\n",
        "    if not CFG[\"trim_punct_on_spans\"]: return s, e\n",
        "    while s < e and text[s] in PUNCT: s += 1\n",
        "    while s < e and text[e-1] in PUNCT: e -= 1\n",
        "    return s, e\n",
        "\n",
        "def _compress_char_runs_base(char_labels: List[str]) -> List[Tuple[int,int,str]]:\n",
        "    spans, i, n = [], 0, len(char_labels)\n",
        "    while i < n:\n",
        "        lab = char_labels[i]\n",
        "        if lab == \"O\": i += 1; continue\n",
        "        j = i + 1\n",
        "        while j < n and char_labels[j] == lab: j += 1\n",
        "        spans.append((i, j, lab))\n",
        "        i = j\n",
        "    return spans\n",
        "\n",
        "def spans_to_charbase(text: str, base_spans: List[Tuple[int,int,str]]) -> List[str]:\n",
        "    arr = [\"O\"] * len(text)\n",
        "    for s, e, base in base_spans:\n",
        "        s0, e0 = max(0,s), min(len(text), e)\n",
        "        if s0 < e0:\n",
        "            arr[s0:e0] = [base] * (e0 - s0)\n",
        "    return arr\n",
        "\n",
        "def _word_labels_from_charbase(text: str, char_labels: List[str]) -> List[Tuple[int,int,str]]:\n",
        "    words = [(m.start(), m.end()) for m in re.finditer(r\"\\S+\", text)]\n",
        "    out, prev_base = [], \"O\"\n",
        "    thr = CFG[\"majority_threshold\"]\n",
        "    for ws, we in words:\n",
        "        cnt, non_o = {}, 0\n",
        "        for i in range(ws, we):\n",
        "            base = char_labels[i] if 0 <= i < len(char_labels) and char_labels[i] in ALLOWED_BASE else \"O\"\n",
        "            cnt[base] = cnt.get(base, 0) + 1\n",
        "            if base != \"O\": non_o += 1\n",
        "\n",
        "        if CFG[\"word_majority\"] and cnt:\n",
        "            best_base, best_cnt = max(cnt.items(), key=lambda x: x[1])\n",
        "            if best_base != \"O\" and best_cnt >= (we - ws) * thr:\n",
        "                base = best_base\n",
        "            else:\n",
        "                if CFG[\"word_inherit_prev\"] and non_o > 0 and prev_base != \"O\" and cnt.get(prev_base, 0) > 0:\n",
        "                    base = prev_base\n",
        "                else:\n",
        "                    base = max(((b,c) for b,c in cnt.items() if b != \"O\"),\n",
        "                               default=(\"O\",0), key=lambda x: x[1])[0]\n",
        "        else:\n",
        "            base = max(((b,c) for b,c in cnt.items() if b != \"O\"),\n",
        "                       default=(\"O\",0), key=lambda x: x[1])[0]\n",
        "\n",
        "        out.append((ws, we, base)); prev_base = base\n",
        "    return out\n",
        "\n",
        "def build_full_word_bio_from_wordlabels(word_labels: List[Tuple[int,int,str]]) -> List[Tuple[int,int,str]]:\n",
        "    out, prev_base, started = [], \"O\", False\n",
        "    for ws, we, base in word_labels:\n",
        "        if base == \"O\":\n",
        "            out.append((ws, we, \"O\")); prev_base, started = \"O\", False\n",
        "        else:\n",
        "            if prev_base == base and started:\n",
        "                out.append((ws, we, f\"I-{base}\"))\n",
        "            else:\n",
        "                out.append((ws, we, f\"B-{base}\")); started = True\n",
        "            prev_base = base\n",
        "    return out\n",
        "\n",
        "def _inject_numeric_overrides(text: str, char_labels: List[str]) -> None:\n",
        "    if not CFG[\"numeric_overrides\"]: return\n",
        "    def mark(a: int, b: int, label: str):\n",
        "        a = max(0, a); b = min(len(char_labels), b)\n",
        "        if a < b: char_labels[a:b] = [label] * (b - a)\n",
        "    for m in RE_PERCENT.finditer(text):  mark(*m.span(), \"PERCENT\")\n",
        "    for m in RE_VOLUME1.finditer(text):  mark(*m.span(), \"VOLUME\")\n",
        "    for m in RE_VOLUME2.finditer(text):  mark(*m.span(), \"VOLUME\")\n",
        "\n",
        "def _apply_margin_rule_per_class(probs_row: np.ndarray) -> bool:\n",
        "    i1 = int(np.argmax(probs_row))\n",
        "    top1 = float(probs_row[i1])\n",
        "    tmp = probs_row.copy(); tmp[i1] = -1.0\n",
        "    top2 = float(tmp.max())\n",
        "\n",
        "    lab = id2label[i1]\n",
        "    base = _base(lab)\n",
        "    delta = CFG.get(\"margin_delta_per_class\", {}).get(base, CFG.get(\"margin_delta\", 0.06))\n",
        "    return (top1 - top2) < delta\n",
        "\n",
        "def predict_char_base(text: str) -> List[str]:\n",
        "    enc = tokenizer(\n",
        "        text,\n",
        "        return_offsets_mapping=True,\n",
        "        return_overflowing_tokens=True,\n",
        "        stride=64,\n",
        "        truncation=True,\n",
        "        max_length=CFG[\"max_len\"],\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    char_labels = [\"O\"] * len(text)\n",
        "    n_chunks = int(enc[\"input_ids\"].shape[0])\n",
        "\n",
        "    for i in range(n_chunks):\n",
        "        offsets = enc[\"offset_mapping\"][i].tolist()\n",
        "        inputs = {\n",
        "            k: v[i:i+1].to(DEVICE)\n",
        "            for k, v in enc.items()\n",
        "            if k in (\"input_ids\", \"attention_mask\", \"token_type_ids\")\n",
        "        }\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits[0]  # [seq, C]\n",
        "        logprobs = torch.log_softmax(logits, dim=-1).cpu().numpy()\n",
        "        probs    = np.exp(logprobs)\n",
        "\n",
        "        keep = [(s,e) for (s,e) in offsets if not (s==0 and e==0)]\n",
        "        if not keep: continue\n",
        "        lp = np.array([lp for lp,(s,e) in zip(logprobs, offsets) if not (s==0 and e==0)])\n",
        "        pr = np.array([pr for pr,(s,e) in zip(probs,    offsets) if not (s==0 and e==0)])\n",
        "\n",
        "        path = lp.argmax(axis=1).tolist()\n",
        "        if CFG[\"use_margin_rule\"]:\n",
        "          for t in range(len(path)):\n",
        "            if _apply_margin_rule_per_class(pr[t]):\n",
        "              path[t] = label2id[\"O\"]\n",
        "\n",
        "        tok_labels = _repair_bio_token_sequence([id2label[int(pid)] for pid in path])\n",
        "        for lab, (s, e) in zip(tok_labels, keep):\n",
        "            if s == e or lab == \"O\": continue\n",
        "            base = _base(lab)\n",
        "            if base == \"O\": continue\n",
        "            s0, e0 = max(0, s), min(len(char_labels), e)\n",
        "            if s0 < e0:\n",
        "                char_labels[s0:e0] = [base] * (e0 - s0)\n",
        "\n",
        "    _inject_numeric_overrides(text, char_labels)\n",
        "    return char_labels\n",
        "\n",
        "def predict_word_bio(text: str) -> List[Tuple[int,int,str]]:\n",
        "    if not text or not text.strip():\n",
        "        return []\n",
        "    char_base = predict_char_base(text)\n",
        "    base_spans = _compress_char_runs_base(char_base)\n",
        "    if base_spans and CFG[\"trim_punct_on_spans\"]:\n",
        "        trimmed = []\n",
        "        for s, e, base in base_spans:\n",
        "            s2, e2 = _trim_punct(text, s, e)\n",
        "            if s2 < e2:\n",
        "                trimmed.append((s2, e2, base))\n",
        "        base_spans = trimmed\n",
        "    char_base2 = spans_to_charbase(text, base_spans)\n",
        "    word_labels = _word_labels_from_charbase(text, char_base2)\n",
        "    full_bio = build_full_word_bio_from_wordlabels(word_labels)\n",
        "\n",
        "    for s,e,lab in full_bio:\n",
        "        assert 0 <= s < e <= len(text)\n",
        "        assert lab == \"O\" or lab.startswith((\"B-\",\"I-\"))\n",
        "    return full_bio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80JJjMN_nrn_"
      },
      "outputs": [],
      "source": [
        "# Блок 8\n",
        "import pandas as pd\n",
        "\n",
        "submission_df = pd.read_csv(\"submission.csv\", sep=\";\", quotechar='\"', engine=\"python\")\n",
        "\n",
        "assert 'submission_df' in globals(), \"Нужно загрузить submission.csv в submission_df\"\n",
        "\n",
        "pred_rows: List[str] = []\n",
        "for text in submission_df[\"sample\"].tolist():\n",
        "    full_bio = predict_word_bio(text)\n",
        "    pred_rows.append(str([(int(s), int(e), str(lab)) for (s,e,lab) in full_bio]))\n",
        "\n",
        "submission_out = submission_df.copy()\n",
        "submission_out[\"annotation\"] = pred_rows\n",
        "submission_out.to_csv(\"submission_out.csv\", sep=\";\", quotechar='\"', index=False)\n",
        "print(\"Готово: submission_out.csv сохранён.\")\n",
        "print(submission_out.head(5).to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
