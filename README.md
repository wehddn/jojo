# X5 NER Service

Сервис извлечения сущностей (TYPE, BRAND, VOLUME, PERCENT) из пользовательских запросов для приложения «Пятёрочка». Поддерживает BIO‑разметку и HTTP‑endpoint `POST /api/predict`.

## Содержание

- [Кратко об архитектуре](#кратко-об-архитектуре)
- [Требования и зависимости](#требования-и-зависимости)
- [Запуск сервиса](#запуск-сервиса)
- [Запуск в Docker](#запуск-в-docker)
- [API](#api)
  - [GET /healthz](#get-healthz)
  - [POST /api/predict](#post-apipredict)
- [Обучение и воспроизводимость](#обучение-и-воспроизводимость)
  - [Данные](#данные)
  - [Шаги обучения](#шаги-обучения)
  - [Ключевые настройки](#ключевые-настройки)
- [Правила инференса (post‑processing)](#правила-инференса-postprocessing)
- [Соответствие ТЗ](#соответствие-тз)
- [Внешние источники и предобученные модели](#внешние-источники-и-предобученные-модели)
- [Тестирование и измерение latency](#тестирование-и-измерение-latency)
- [Структура репозитория](#структура-репозитория)

---

## Кратко об архитектуре

```
[HTTP клиент] -> FastAPI (main.py)
   |-- при старте: load_model(MODEL_DIR)
   |-- endpoint /api/predict -> ner_runtime.predict_word_bio()-> токенизация -> инференс -> пост‑правила (margin/regex)
```

- **FastAPI сервис**: [main.py](main.py) - точка входа веб‑приложения. Основной endpoint /api/predict принимает JSON с полем "input" (строка), вызывает функцию инференса и возвращает массив JSON-объектов со спанами BIO по словам.
- **Ядро инференса**: [ner_runtime.py](ner_runtime.py) - загрузка модели/токенайзера и полный пайплайн разметки BIO по словам.
- **Обучение**: [x5.ipynb](src/x5.ipynb) ([версия .py](src/x5.py)) - скрипт‑ноутбук для тренировки модели (HuggingFace Transformers), сохранение весов в `./model`.
- **Данные/синтетика**: [gen_synt.py](src/gen_synt.py) - воспроизводимое добавление синтетики (опечатки брендов + числовые шаблоны) в `synthetic_train.csv`.

> Более подробно архитектура описана в [docs/tech_report.md](docs/tech_report.md).
---

## Требования и зависимости

**Аппаратные**
- CPU x86_64, 4 vCPU, 8 GB RAM достаточно для продакшн‑инференса.
- GPU не обязателен. Параллелизм - через несколько воркеров uvicorn.

**ПО**
- Python ≥ 3.10
- Рекомендуемая среда: сервис кроссплатформенный, тестировался на Linux (Ubuntu 22.04+).

**Python‑зависимости** :
```txt
fastapi
uvicorn[standard]
transformers>=4.36
numpy
accelerate
seqeval
torch
```

> В requirements.txt также используется параметр `--extra-index-url https://download.pytorch.org/whl/cpu` для установки torch без поддержки GPU

---

## Запуск сервиса локально (без Docker)

1. **Установите зависимости**
   ```bash
   python -m venv .venv && source .venv/bin/activate
   pip install -U pip
   pip install -r requirements.txt
   ```

2. **Положите модель** в папку `./model`. По умолчанию сервис ищет веса именно там.

3. **Запустите сервис**
   ```bash
   uvicorn main:app --host 0.0.0.0 --port 8000 --workers 2
   ```

4. **Проверка состояния**
   ```bash
   curl http://127.0.0.1:8000/healthz
   ```

5. **Пример запроса (локально)**
   ```bash
   curl -X POST "http://127.0.0.1:8000/api/predict" -H "Content-Type: application/json; charset=utf-8" -d "{\"input\":\"молоко\"}"
   ```
   Пример ответа:
   ```json
   [{"start_index":0,"end_index":6,"entity":"B-TYPE"}]
   ```
   Пустой ввод `""` возвращает `[]`.

---

## Запуск в Docker

**Сборка образа**
```bash
docker build -t x5-ner:latest .
```

Запуск :
```bash
docker run -d --name x5-ner -p 8000:8000 --restart=always x5-ner:latest
```
---

## API

### `GET /healthz`
Пример ответа:
```json
{"status":"ok","model_dir":"/app/model","latest_update":"2025-09-30"}
```

### `POST /api/predict`
Тело запроса:
```json
{"input":"сгущенное молоко"}
```
Ответ - список спанов (BIO по словам):
```json
[
  {"start_index":0, "end_index":9,  "entity":"B-TYPE"},
  {"start_index":10,"end_index":16, "entity":"I-TYPE"}
]
```

Тело запроса с пустой строкой:
```json
{"input":""}
```

Ответ - пустой список:
```json
[]
```


Гарантии:
- Порядок спанов соответствует порядку слов во входной строке.
- Границы соответствуют индексам `str[start:end]` исходного текста.
- Пустая строка - пустой список.

---

## Обучение и воспроизводимость

### Данные
- [train.csv](src/train.csv) - исходная обучающая выборка организаторов (колонки `sample;annotation`).
- [submission.csv](src/submission.csv) - вспомогательный файл для валидации пайплайна.

### Шаги обучения
1. Сгенерируйте синтетику (устойчивость к опечаткам + числовые паттерны):
   ```bash
   python gen_synt.py
   ```
   Результат - `synthetic_train.csv` (4000 строк). Итоговый датасет - объединение с `train.csv`.

2. **Запустите обучение** (используется `cointegrated/rubert-tiny2`):
   ```bash
   python x5.py
   ```
   Либо в Jupyter‑ноутбуке [x5.ipynb](src/x5.ipynb).

   Чекпойнт и токенайзер можно сохранить в `./model`, для этого раскомментируйте соответствующие строки в файле.

### Ключевые настройки
- BIO‑метки: `O, B-*, I-*` (TYPE, BRAND, VOLUME, PERCENT).
- Токен‑лейблинг по пересечению оффсетов с аннотациями.
- Валидация: seqeval F1 (IOB2).
- Сид: 42, сплит 90/10.

---

## Правила инференса (post‑processing)

- **Margin rule per class**: если разница Top‑1/Top‑2 ниже порога, токен понижается в `O`. Пороги различаются для базовых классов (TYPE/BRAND выше, VOLUME/PERCENT ниже).
- **Numeric overrides**: регулярные выражения для `VOLUME` (единицы измерения, пачки, формулы вида `2x6 шт`) и `PERCENT` (`\d+(,\d+)?%`).
- **Trim punctuation**: обрезка пунктуации на краях спанов.
- **Агрегация по словам**: мажоритарка внутри слова с мягким наследованием класса от предыдущего слова; затем восстановление `B-/I-` по непрерывным сегментам.

Все правила отключаемы через `CFG` при необходимости.

---

## Соответствие ТЗ

- Поддержка сущностей TYPE/BRAND/VOLUME/PERCENT с BIO.
- `POST /api/predict` c требуемым форматом ответа.
- Пустой ввод - пустой список.
- Отсутствие авторизации для доступа к API.
- Для тренировки используются только предоставленные данные + синтетика (без ручных словарей). В качестве предобученной модели используется [cointegrated/rubert-tiny2](https://huggingface.co/cointegrated/rubert-tiny2) с лицензией MIT.
- Устойчивость к опечаткам: обучено на синтетике (искажения брендов), правила не используют закрытых словарей.
- Быстродействие: быстрое время ответа благодаря легковесной модели и возможной оптимизации (несколько воркеров uvicorn).
- Упаковка в Docker.
- Документация по развёртыванию.

---

## Внешние источники и предобученные модели

- **HuggingFace Transformers** + предобученная модель [cointegrated/rubert-tiny2](https://huggingface.co/cointegrated/rubert-tiny2) для русского языка (дообучается под задачу NER).
- **Отсутствуют** ручные словари по брендам/типам; используются только регулярные выражения для числовых сущностей (разрешено ТЗ). Данные синтетики генерируются кодом и воспроизводимы.

---

## Тестирование и измерение latency

- Функциональные проверки: `curl` на `/api/predict` и `/healthz`.
- Оффлайн‑проверка пайплайна на `submission.csv` в Telegram‑чате команды.
- Онлайн‑оценка пайплайна в Telegram‑чате команды: CPU, 4 vCPU, 8 GB RAM - F1 0.9210; Latency 187ms.
---

## Структура репозитория

```
.
main.py                # FastAPI приложение (эндпойнты /healthz, /api/predict)
ner_runtime.py         # загрузка и инференс модели, пост‑обработка

requirements.txt       # Python зависимости
.dockerignore          # исключения для Docker сборки
Dockerfile             # образ сервиса (uvicorn + модель)

model/                 # папка с сохранённой моделью/токенайзером

src/                   # исходные файлы для обучения и генерации данных
- gen_synt.py          # генерация synthetic_train.csv
- synthetic_train.csv  # синтетические данные для обучения
- x5.ipynb             # Jupyter ноутбук для обучения
- x5.py                # копия x5.ipynb в .py формате
```

---
