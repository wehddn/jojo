Этот документ дополняет README и подробно описывает три ключевых части решения: генерацию синтетики, обучение модели и пост‑обработку.

---

## 1) Генерация синтетики

### 1.1. Цель

Повысить устойчивость модели к опечаткам (в брендах и типах), латинице/кириллице и разнообразным форматам объёмов/процентов. Синтетика не использует закрытые словари - всё воспроизводимо кодом.

### 1.2. Скрипт и входные данные

- Скрипт: `src/gen_synt.py`
- Входные данные: `train.csv` (`sample;annotation`).
- Результат: `synthetic_train.csv` (та же схема `sample;annotation`).

### 1.3. Метод

1. Разбор исходной разметки - парсинг `annotation` в список спанов. Функция собирает полный BIO со вставкой O‑отрезков, чтобы покрыть весь текст (нет "дыр" между сущностями).
2. Извлечение брендов: из исходных спанов выбираются текстовые фрагменты меток `B/​I‑BRAND`.
3. Инъекция опечаток (для части брендов, с ограничением не более *K* на бренд):
   - удаление/вставка/замена символа;
   - сосед по раскладке клавиатуры (`о-л`, `а-ф` и т. п.);
   - обмен местами двух соседних символов.
4. Пересчёт индексов аннотаций: после замены бренда в тексте индексы спанов корректируются, добавляется один новый спан для искажённого бренда.
5. Числовые сущности (VOLUME/PERCENT):
   - добавление спанов для шаблонов вида `500 мл`, `2x6 шт`, `1,5 л`, `750 г` и т. п.;
   - проценты: `r"(?<!\w)(\d{1,3}(?:[.,]\d{1,2})?)\s*%"`.
6. Слияние спанов: числовые спаны добавляются, не перекрывая уже существующие; итог сортируется и конвертируется в полный список с `O`‑отрезками.

### 1.4. Контроль качества синтетики

- Ограничение на общее число примеров и на число примеров на бренд (во избежание переобучения на артефакты).

### 1.5. Воспроизводимость

- Фиксирован `SEED=42` (и для `random`, и для `numpy`).
- Команда для запуска:
  ```bash
  python src/gen_synt.py  # создаст synthetic_train.csv
  ```

---

## 2) Обучение модели

### 2.1. База и окружение

- Базовая модель: [cointegrated/rubert-tiny2](https://huggingface.co/cointegrated/rubert-tiny2) - доступна по лицензии MIT.
- Токенизация: стандартный токенайзер базовой модели.
- Метрика: seqeval F1 (IOB2).
- Железо: обучение проводилось на GPU T4 в Google Colab.

### 2.2. Данные

- Тренировочные данные: `train.csv` с дополнением `synthetic_train.csv`.
- Сплит: 90/10 с фиксированным `seed=42`.
- Выравнивание токен‑лейблов: разметка на токены по пересечению оффсетов; вне сущностей — `O` (или `-100` для игнорируемых субтокенов в лоссе).

### 2.3. Ключевые гиперпараметры

Подбор гиперпараметров носил прикладной характер: варьировались `learning_rate` (2e-5…5e-5), `batch_size` (8/16/32) и число эпох (8…15). Каждая конфигурация обучалась с фиксированным seed на 90/10-валидации; качество измерялось по F1 в офлайн-оценке (submission.csv через Telegram). Итоговый набор выбран по наилучшему F1.

- `learning_rate=3e-5`
- `batch_size_train=16`, `batch_size_eval=16`
- `num_train_epochs=12`
- `weight_decay=0.01`
- `save_strategy="epoch"`, `eval_strategy="epoch"`, `logging_strategy="steps"`, `logging_steps=50`
- `load_best_model_at_end=True`, `metric_for_best_model="f1"`, `greater_is_better=True`, `save_total_limit=3`
- `EarlyStoppingCallback(patience=3, metric="f1")`.

### 2.4. Экспорт чекпойнта

- Сохранение токенайзера и весов в директорию `model/`.
- Файлы: `config.json`, `pytorch_model.bin` или `model.safetensors`, `tokenizer.json`, `special_tokens_map.json`, `tokenizer_config.json`, `label2id/id2label`.

### 2.5. Команда для запуска обучения

```bash
python src/x5.py
```

(или запуск ноутбука `src/x5.ipynb`; строка `!pip ...` в .py‑версии не требуется и закомментирована.)

---

## 3) Пост‑обработка (правила)

### 3.1. Обзор

После argmax‑предсказаний на токенах выполняется посимвольное восстановление и свёртка в базовые классы (TYPE, BRAND, VOLUME, PERCENT), затем агрегация по словам и финальная сборка BIO.

Используемые правила : 

- Регулярные выражения покрывают устойчивые, "правилоподобные" случаи.
- Поклассовые margin rules уменьшают вероятность ошибок при различении TYPE - BRAND на "пограничных" токенах.
- Мажоритарное правило по словам с наследованием делают BIO устойчивее на опечатках/субтокенах и при смешанной кириллице/латинице внутри одного слова.

### 3.2. Trim punctuation

- Обрезка пунктуации на краях выделенных спанов (`;:,.!?()[]{}«»"'—–-`).

### 3.3. Numeric overrides

- Регулярные выражения распознают:
  - PERCENT: числа с `,`/`.` и `%` (например, `12%`, `7,5%`).
  - VOLUME: набор единиц измерения: `мл|л|литр(а|ов)?|г|гр|грамм(а|ов)?|кг|шт|уп|упак|бут(ылка|ки|ок)|табл|таб|капс|порц|пак`, а также формы `NxM шт/уп/…` и варианты с точками (`л.`/`мл.`/`шт.` и пр.).
- При совпадении регулярного выражения соответствующий диапазон помечается как `BASE=VOLUME`/`PERCENT` и перекрывает менее уверенные метки.

### 3.4. Margin rule (per‑class)

- Если `(top1 - top2) < delta`, токен понижается в `O`.
- Глобальный `delta` и классовые дельты:
  - `delta_global = 0.06`
  - `TYPE=0.07`, `BRAND=0.07`, `VOLUME=0.02`, `PERCENT=0.015`
- Цель: уменьшить ложные срабатывания по TYPE/BRAND и сохранить чувствительность к числовым сущностям.

### 3.5. Агрегация по словам

- Для каждого слова собирается посимвольная "база" и применяется мажоритарное правило:
  - порог доли лучшего класса в слове `majority_threshold = 0.58`;
  - если порог не набран, и в слове есть не‑`O` символы, можно унаследовать класс предыдущего слова (`word_inherit_prev=True`), если он встречается в текущем слове.
- Затем диапазоны объединяются и восстанавливаются BIO‑теги: непрерывные сегменты одного BASE → `B-*/I-*`.

### 3.6. Производительность и стабильность

- `torch.set_num_threads(1)` - стабилизация latency на CPU.
- Параллелизм достигается количеством uvicorn‑воркеров (каждый воркер держит собственную копию модели).
